{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport nltk\nfrom wordcloud import WordCloud\nfrom nltk.stem.porter import *\nfrom sklearn.model_selection import train_test_split\n\n\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T18:36:20.766397Z","iopub.execute_input":"2022-05-14T18:36:20.766827Z","iopub.status.idle":"2022-05-14T18:36:22.231558Z","shell.execute_reply.started":"2022-05-14T18:36:20.766736Z","shell.execute_reply":"2022-05-14T18:36:22.230838Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"xx=pd.read_csv(\"../input/twitter-sentiment-analysis-tah/train.csv\")\ntrain_target=xx.drop(['date', 'flag', 'user', 'text', 'id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:37:18.680380Z","iopub.execute_input":"2022-05-14T18:37:18.680640Z","iopub.status.idle":"2022-05-14T18:37:24.122840Z","shell.execute_reply.started":"2022-05-14T18:37:18.680612Z","shell.execute_reply":"2022-05-14T18:37:24.121961Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"../input/twitter-sentiment-analysis-tah/train.csv\").drop([\"target\", \"flag\"],axis=1)\ntest=pd.read_csv(\"../input/twitter-sentiment-analysis-tah/test.csv\").drop(\"flag\",axis=1)\nsample_submission=pd.read_csv(\"../input/twitter-sentiment-analysis-tah/sample_submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:37:24.124269Z","iopub.execute_input":"2022-05-14T18:37:24.124540Z","iopub.status.idle":"2022-05-14T18:37:29.298325Z","shell.execute_reply.started":"2022-05-14T18:37:24.124505Z","shell.execute_reply":"2022-05-14T18:37:29.297589Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Tweets Preprocessing and Cleaning","metadata":{}},{"cell_type":"code","source":"df_train=train.copy()\ndf_test=test.copy()\n\n\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt  \n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\n# Removing (@user)\ndf_train['text'] = np.vectorize(remove_pattern)(df_train['text'], \"@[\\w]*\")\ndf_test['text'] = np.vectorize(remove_pattern)(df_test['text'], \"@[\\w]*\")\n\n\n#Remove URL \n\ndf_train['text'] = df_train['text'].apply(lambda x: remove_URL(x))\ndf_test['text']=df_test['text'].apply(lambda x: remove_URL(x))\n\n# Removing Punctuations, Numbers, and Special Characters\ndf_train['text'] = df_train['text'].str.replace(\"[^a-zA-Z#]\", \" \")\ndf_test['text']=df_test['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n\n\n#Removing Short Words \ndf_train['text'] = df_train['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndf_test['text'] = df_test['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n\ndummy=pd.DataFrame(index=[df_train['text'],train[\"text\"]])\ndummy.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:37:29.299664Z","iopub.execute_input":"2022-05-14T18:37:29.299942Z","iopub.status.idle":"2022-05-14T18:38:49.105208Z","shell.execute_reply.started":"2022-05-14T18:37:29.299904Z","shell.execute_reply":"2022-05-14T18:38:49.104538Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Tokenization\ntokenized_text =df_train['text'].apply(lambda x: x.split())\ntokenized_text_test=df_test['text'].apply(lambda x: x.split())\n# Stemming \n\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\ntokenized_text1 = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_text2 = tokenized_text_test.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n\ndf_train['text']=tokenized_text1\ndf_test['text']=tokenized_text2","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:38:49.106728Z","iopub.execute_input":"2022-05-14T18:38:49.107051Z","iopub.status.idle":"2022-05-14T18:43:56.847014Z","shell.execute_reply.started":"2022-05-14T18:38:49.107014Z","shell.execute_reply":"2022-05-14T18:43:56.846215Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# from nltk.stem import LancasterStemmer\n# lancaster=LancasterStemmer()\n# lancaster_tokenized_text = tokenized_text.apply(lambda x: [lancaster.stem(i) for i in x]) # stemming\n# lancaster_tokenized_text_test=tokenized_text_test.apply(lambda x: [lancaster.stem(i) for i in x]) # stemming\n\n# df_train['text']=lancaster_tokenized_text\n# df_test['text']=lancaster_tokenized_text_test\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T23:09:03.34901Z","iopub.execute_input":"2022-04-16T23:09:03.349265Z","iopub.status.idle":"2022-04-16T23:12:50.060358Z","shell.execute_reply.started":"2022-04-16T23:09:03.34923Z","shell.execute_reply":"2022-04-16T23:12:50.059526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# For reversing the Stream:\n\n\nfor i in range(len(tokenized_text1)):\n    tokenized_text1[i] = ' '.join(tokenized_text1[i])\n\ndf_train['text'] = tokenized_text1\n\n\n\nfor i in range(len(tokenized_text2)):\n    tokenized_text2[i] = ' '.join(tokenized_text2[i])\n\ndf_test['text'] = tokenized_text2\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:43:56.848281Z","iopub.execute_input":"2022-05-14T18:43:56.848528Z","iopub.status.idle":"2022-05-14T18:44:28.968834Z","shell.execute_reply.started":"2022-05-14T18:43:56.848497Z","shell.execute_reply":"2022-05-14T18:44:28.968082Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train['text'].shape,df_test['text'].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:56:05.410702Z","iopub.execute_input":"2022-04-17T14:56:05.41125Z","iopub.status.idle":"2022-04-17T14:56:05.416621Z","shell.execute_reply.started":"2022-04-17T14:56:05.411215Z","shell.execute_reply":"2022-04-17T14:56:05.415917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  WordCloud\nUnderstanding the **common words** used in the text:","metadata":{}},{"cell_type":"code","source":"all_words = ' '.join([text for text in df_train['text'] ])\nwordcloud = WordCloud(width=800, height=500, background_color ='white',random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:44:28.970732Z","iopub.execute_input":"2022-05-14T18:44:28.971078Z","iopub.status.idle":"2022-05-14T18:45:22.419070Z","shell.execute_reply.started":"2022-05-14T18:44:28.971039Z","shell.execute_reply":"2022-05-14T18:45:22.418346Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#  Hashtags impact ","metadata":{}},{"cell_type":"code","source":"def hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags\n\nHT_regular = hashtag_extract(df_train['text'])\nHT_regular = sum(HT_regular,[])\n\n\n\n# Train Trends\n\na = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()\n\n# Test Trends\n\n\nHT_regular_test = hashtag_extract(df_test['text'])\nHT_regular_test = sum(HT_regular_test,[])\n\na = nltk.FreqDist(HT_regular_test)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:45:22.420014Z","iopub.execute_input":"2022-05-14T18:45:22.420242Z","iopub.status.idle":"2022-05-14T18:46:53.647295Z","shell.execute_reply.started":"2022-05-14T18:45:22.420210Z","shell.execute_reply":"2022-05-14T18:46:53.646329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"nltk.FreqDist(HT_regular)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:57:27.885716Z","iopub.execute_input":"2022-04-17T22:57:27.885972Z","iopub.status.idle":"2022-04-17T22:57:27.917631Z","shell.execute_reply.started":"2022-04-17T22:57:27.885943Z","shell.execute_reply":"2022-04-17T22:57:27.916959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.FreqDist(HT_regular_test)['irememb']","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:58:30.823399Z","iopub.execute_input":"2022-04-17T22:58:30.823935Z","iopub.status.idle":"2022-04-17T22:58:30.836917Z","shell.execute_reply.started":"2022-04-17T22:58:30.823897Z","shell.execute_reply":"2022-04-17T22:58:30.836036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Features","metadata":{}},{"cell_type":"markdown","source":"**techniques** :\n\n1. Bag-of-Words Features\n2. TF-IDF\n3. Word Embeddings","metadata":{}},{"cell_type":"code","source":"\"\"\"\nBag-of-Words Features (BoW)\n\"\"\"\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# bag-of-words feature matrix\nbow_train= bow_vectorizer.fit_transform(df_train['text'])\nbow_test= bow_vectorizer.fit_transform(df_test['text'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:51:46.320698Z","iopub.execute_input":"2022-04-17T15:51:46.320946Z","iopub.status.idle":"2022-04-17T15:52:04.4846Z","shell.execute_reply.started":"2022-04-17T15:51:46.320914Z","shell.execute_reply":"2022-04-17T15:52:04.483713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:52:04.486206Z","iopub.execute_input":"2022-04-17T15:52:04.486483Z","iopub.status.idle":"2022-04-17T15:52:04.491993Z","shell.execute_reply.started":"2022-04-17T15:52:04.486446Z","shell.execute_reply":"2022-04-17T15:52:04.491307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(bow_train)\npred = kmeans.predict(bow_test.toarray())\n\nsub = pd.DataFrame(columns=['id','target'])\nsub['id']=test[\"id\"]\nsub[\"target\"]=pred\nsub.to_csv('sub3.csv' , index = False)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:59:36.651668Z","iopub.execute_input":"2022-04-17T14:59:36.651932Z","iopub.status.idle":"2022-04-17T14:59:51.703432Z","shell.execute_reply.started":"2022-04-17T14:59:36.651903Z","shell.execute_reply":"2022-04-17T14:59:51.702569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AffinityPropagation\n\nx_train, x_val, y_train, y_val = train_test_split(bow_train, train_target, random_state=42, test_size=0.3)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:52:32.081224Z","iopub.execute_input":"2022-04-17T15:52:32.081516Z","iopub.status.idle":"2022-04-17T15:52:32.376466Z","shell.execute_reply.started":"2022-04-17T15:52:32.081486Z","shell.execute_reply":"2022-04-17T15:52:32.375612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclustering = AffinityPropagation(random_state=45)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:52:47.81906Z","iopub.execute_input":"2022-04-17T15:52:47.819354Z","iopub.status.idle":"2022-04-17T15:52:47.823743Z","shell.execute_reply.started":"2022-04-17T15:52:47.819322Z","shell.execute_reply":"2022-04-17T15:52:47.82261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clustering.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:52:58.835306Z","iopub.execute_input":"2022-04-17T15:52:58.835612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=clustering.predict(x_val)\nf1_score(y_val, pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import AffinityPropagation\n# from sklearn.cluster import AgglomerativeClustering\n# from sklearn.cluster import Birch\n# from sklearn.cluster import DBSCAN\n# from sklearn.cluster import MiniBatchKMeans\n# from sklearn.cluster import MeanShift\n# from sklearn.cluster import OPTICS\n# from sklearn.cluster import SpectralClustering\n# from sklearn.mixture import GMM  #Gaussian Mixture Models\n\n\n\n\n\n# model1=AffinityPropagation(random_state=45).fit(x_train)\n# model2=AgglomerativeClustering().fit(x_train)\n# model3=Birch(branching_factor=100, threshold=.5).fit(x_train)\n# model4=DBSCAN(eps=3, min_samples=2).fit(x_train)\n# model5=MiniBatchKMeans(n_clusters=2, random_state=0, batch_size=6).fit(x_train)\n# model6=MeanShift(bandwidth=2).fit(x_train)\n# model7=OPTICS(min_samples=2).fit(x_train)\n# model8=SpectralClustering(n_clusters=2,assign_labels='discretize',random_state=45).fit(x_train)\n# model9=GMM(n_components=2).fit(x_train)\n\n\n# pred1=model1.predict(x_val)\n# pred2=model2.predict(x_val)\n# pred3=model3.predict(x_val)\n# pred4=model4.predict(x_val)\n# pred5=model5.predict(x_val)\n# pred6=model6.predict(x_val)\n# pred7=model7.predict(x_val)\n# pred8=model8.predict(x_val)\n\n\n\n# print(f1_score(y_val, pred1))  \n# print(f1_score(y_val, pred2))    \n# print(f1_score(y_val, pred3))    \n# print(f1_score(y_val, pred4))    \n# print(f1_score(y_val, pred5))    \n# print(f1_score(y_val, pred6))    \n# print(f1_score(y_val, pred7))    \n# print(f1_score(y_val, pred8))    \n# print(f1_score(y_val, pred9))    \n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTF-IDF Features\n\nTerm frequency–inverse document frequency,\nis a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.”\n\nTF: It is a measure of how frequently a term, t, appears in a document, d\nIDF : is a measure of how important a term is\n\n\"\"\"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(df_train['text'])\ntfidf_test = tfidf_vectorizer.fit_transform(df_test['text'])","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:18:44.696261Z","iopub.execute_input":"2022-04-16T22:18:44.696575Z","iopub.status.idle":"2022-04-16T22:19:02.768194Z","shell.execute_reply.started":"2022-04-16T22:18:44.69654Z","shell.execute_reply":"2022-04-16T22:19:02.767412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building model using TF-IDF features**","metadata":{}},{"cell_type":"markdown","source":"# Splitting ","metadata":{}},{"cell_type":"code","source":"\ntrain_bow = bow_train[:1279999,:]\ntest_bow = bow_test[320000:,:]\n\n# x_train, x_val, y_train, y_val = train_test_split(train_bow, train_target, random_state=42, test_size=0.3)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:54:02.536333Z","iopub.execute_input":"2022-04-16T22:54:02.537092Z","iopub.status.idle":"2022-04-16T22:54:14.913069Z","shell.execute_reply.started":"2022-04-16T22:54:02.537045Z","shell.execute_reply":"2022-04-16T22:54:14.912355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_bow.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:56:14.782608Z","iopub.execute_input":"2022-04-16T22:56:14.783196Z","iopub.status.idle":"2022-04-16T22:56:14.788738Z","shell.execute_reply.started":"2022-04-16T22:56:14.783151Z","shell.execute_reply":"2022-04-16T22:56:14.78781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling ","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import f1_score\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(x_train)\n\n#predictions from kmeans\npred = kmeans.predict(x_val)\n\nf1_score(y_val, pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:18:14.377864Z","iopub.execute_input":"2022-04-16T22:18:14.37814Z","iopub.status.idle":"2022-04-16T22:18:22.592167Z","shell.execute_reply.started":"2022-04-16T22:18:14.378108Z","shell.execute_reply":"2022-04-16T22:18:22.59141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_tfidf = tfidf[:1279999,:]\ntest_tfidf = tfidf_test[320000:,:]\n\n# x_train, x_val, y_train, y_val = train_test_split(train_tfidf, train_target, random_state=42, test_size=0.3)\n\n# kmeans.fit(x_train)\n\n# #predictions from kmeans\n# pred = kmeans.predict(x_val)\n\n# f1_score(y_val, pred) \ntest_tfidf.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:39:06.914837Z","iopub.execute_input":"2022-04-16T22:39:06.915509Z","iopub.status.idle":"2022-04-16T22:39:06.950652Z","shell.execute_reply.started":"2022-04-16T22:39:06.915447Z","shell.execute_reply":"2022-04-16T22:39:06.949365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_test.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:46:44.524799Z","iopub.execute_input":"2022-04-16T22:46:44.525621Z","iopub.status.idle":"2022-04-16T22:46:46.83805Z","shell.execute_reply.started":"2022-04-16T22:46:44.525577Z","shell.execute_reply":"2022-04-16T22:46:46.837331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.fit(train_tfidf)\npred = kmeans.predict(tfidf_test.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:47:03.709265Z","iopub.execute_input":"2022-04-16T22:47:03.709555Z","iopub.status.idle":"2022-04-16T22:47:26.333432Z","shell.execute_reply.started":"2022-04-16T22:47:03.709521Z","shell.execute_reply":"2022-04-16T22:47:26.332602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(columns=['id','target'])\nsub['id']=test[\"id\"]\nsub[\"target\"]=pred\nsub.to_csv('sub1.csv' , index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:48:12.474535Z","iopub.execute_input":"2022-04-16T22:48:12.475646Z","iopub.status.idle":"2022-04-16T22:48:12.959537Z","shell.execute_reply.started":"2022-04-16T22:48:12.475594Z","shell.execute_reply":"2022-04-16T22:48:12.958747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[\"target\"].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:48:01.33958Z","iopub.execute_input":"2022-04-16T22:48:01.340266Z","iopub.status.idle":"2022-04-16T22:48:01.348835Z","shell.execute_reply.started":"2022-04-16T22:48:01.340208Z","shell.execute_reply":"2022-04-16T22:48:01.348035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}